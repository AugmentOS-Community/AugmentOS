{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_317_large...\n",
      "Processing episode_211_large...\n",
      "Processing episode_186_large...\n",
      "Processing episode_242_large...\n",
      "Processing episode_061_large...\n",
      "Processing episode_149_large...\n",
      "Processing episode_153_large...\n",
      "Processing episode_015_large...\n",
      "Processing episode_290_large...\n",
      "Processing episode_191_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" And if there was a really shitty job to do, he'd say, see the job, do the job, stay out of the misery. Just don't indulge any negativity, do the things that need done. And so there's like, there's an empowerment and a nobility together. And yeah, extraordinarily fortunate. Is there ways you think you could have been a better son?, Is there things you regret? Interesting question. Let me first say, just as a bit of a criticism, that what kind of man do you think you are not wearing a suit and tie, if a real man should? Exactly I agree with your dad on that point., You mentioned offline that he suggested a real man should wear a suit and tie. But outside of that, is there ways you could have been a better son? Maybe next time on your show, I'll wear a suit and tie. My dad would be happy about that., I can answer the question later in life, not early. I had just a huge amount of respect and reverence for my dad when I was young. So I was asking myself that question a lot. So there weren't a lot of things I knew that I wasn't seeking to apply. There was a phase when I went through my kind of individuation, differentiation, where I, had to make him excessively wrong about too many things. I don't think I had to, but I did. And he had a lot of kind of nonstandard model beliefs about things, whether early kind of ancient civilizations or ideas on evolutionary theory or alternate models of physics.\\n And they weren't irrational, but they didn't all have the standard of epistemic proof that I would need. And I went through, and some of them were kind of spiritual ideas as well, I went through a phase in my early 20s where I kind of had the attitude that Dawkins or a Christopher\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=20)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective: \n",
    "Identify \"Rare Entities\" in a conversation transcript. These include rare words, phrases, jargons, adages, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. We are using a really lousy transcribing service, so words are often mispelt, but you can autocorrect and piece together implied entities that are described in the conversation context but not explicitly mentioned, your vast knowledge base to derive the \"Rare Entity\" originally mentioned by the user. If you feel the need to search for the entity, then it is most likely mistranscribed.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance:\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose terms with non-obvious meanings, such as \"Butterfly Effect\" and not \"Electric Car\".\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Searchability: Likely to have a specific and valid reference source: Wikipedia page, dictionary entry etc.\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array:\n",
    "entities: [{{ name: string, definition: string, ekg_search_keyword: string }}], where definition is concise (< 12 words), and ekg_search_keyword as the best search keyword for the Google Knowledge Graph.  \n",
    "\n",
    "## Additional Guidelines:\n",
    "- Entity names should be quoted from the conversation, so the output definitions can be referenced back to the conversation, unless they are transcribed wrongly, then use the official name.\n",
    "- For the search keyword, use complete, official and context relevant keyword(s) to search for that entity. You might need to autocomplete entity names or use their official names or add additional context keywords to help with searchability, especially if the entity is ambiguous or has multiple meanings. For rare words, include \"definition\" in the search keyword.\n",
    "- Definitions should use simple language to be easily understood.\n",
    "- Select entities whose definitions you are very confident about, otherwise skip them.\n",
    "- Multiple entities can be detected from one phrase, for example, \"The Lugubrious Game\" can be defined as a painting, and the rare word \"lugubrious\" is also worth defining.\n",
    "- Limit results to 4 or less entities, prioritize rarity.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: If the conversation talks about \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer\", but the entity name would be \"Balmer\" because that is the name quoted from the conversation.\n",
    "    - Replacing unofficial name example: If the conversation talks about \"Clay Institute\", the keyword is \"Clay Mathematics Institute\" since that is the official name, but the entity name would be \"Clay Institute\" because that is the name quoted from the conversation.\n",
    "    - Adding context example: If the conversation talks about \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything (concept)\", because there is a popular movie with the same name. \n",
    "\n",
    "## Recent Definitions:\n",
    "These have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"Moore's Law\", definition: \"Computing power doubles every ~2 yrs\", ekg_search_key: \"Moore's Law\" }}, {{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", ekg_search_key: \"Pareto Principle\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If no relevant entities are identified, output empty arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    pass\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    ekg_search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on Google Enterprise Knowledge Graph\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in NLP\",\n",
      "      \"ekg_search_keyword\": \"Hugging Face\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research laboratory\",\n",
      "      \"ekg_search_keyword\": \"OpenAI\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"Large language machine learning model\",\n",
      "      \"ekg_search_keyword\": \"Falcon LLM model\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in NLP', ekg_search_keyword='Hugging Face'), Entity(name='OpenAI', definition='AI research laboratory', ekg_search_keyword='OpenAI'), Entity(name='Falcon LLM model', definition='Large language machine learning model', ekg_search_keyword='Falcon LLM model')]\n"
     ]
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
