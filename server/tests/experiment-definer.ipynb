{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_250_large...\n",
      "Processing episode_217_large...\n",
      "Processing episode_085_large...\n",
      "Processing episode_201_large...\n",
      "Processing episode_142_large...\n",
      "Processing episode_088_large...\n",
      "Processing episode_078_large...\n",
      "Processing episode_241_large...\n",
      "Processing episode_140_large...\n",
      "Processing episode_057_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That, to me, is an interesting application of a humanoid form because humans are drawn, like I mentioned to you, like robots, we're drawn to legs and limbs and body language and all that kind of stuff. And even a face, even if you don't have the facial features, which you might not want to have to reduce the creepiness factor, all that kind of stuff. But yeah, that, to me, the humanoid form is compelling., But in terms of that being the right form for the factory environment, I'm not so sure. Yeah, for the factory environment, like right off the bat, what are you optimizing for? Is it strength? Is it mobility? Is it versatility, right? Like that changes completely the look and feel of the robot that you create, you know, and almost certainly the human form is over designed for some dimensions and constrained for some dimensions., And so, like, what are you grasping? Is it big? Is it little, right? So you would customize it and make it customizable for the different needs if that was the optimization, right? And then, you know, for the other one, I could totally be wrong. You know, I still feel that the closer you try to get to a human, the more you're subject to the biases of what a human should be, and you lose flexibility to shift away from your weaknesses and towards your strengths. And that changes over time, but there's ways to make really approachable and natural interfaces for robotic kind of characters and, you know, kind of deployments in these applications that do not at all look like a human directly,, but that actually creates way more flexibility and capability and role and forgiveness and interface and everything else. Yeah, it's interesting, but I'm still confused by the magic I see in legged robots. Yeah, so there is a magic. So I'm absolutely amazed at it from a technical curiosity standpoint and like the magic that like the Boston Dynamics team can do from, you know, like from walking and jumping and so forth.\\n Now, like there's been a long journey to try to find an application for that sort of technology. But wow, that's incredible technology, right? So then you kind of go towards, OK, are you working back from a goal of what you're trying to solve? Are you working forward from a technology and I'm looking for a solution? And I think that's where it's a kind of a bi directional search oftentimes, but you got the two have to meet.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=20)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective: \n",
    "Identify \"Rare Entities\" in a conversation transcript. These include rare words, phrases, jargons, adages, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. You can also intelligently detect entities that are described in the conversation but not explicitly mentioned.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance:\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose terms with non-obvious meanings, such as \"Butterfly Effect\" and not \"Electric Car\".\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Searchability: Likely to have a specific and valid reference source: Wikipedia page, dictionary entry etc.\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array:\n",
    "entities: [{{ name: string, definition: string, ekg_search_keyword: string }}], where definition is concise (< 12 words), and ekg_search_keyword as the best search keyword for the Google Knowledge Graph.  \n",
    "\n",
    "## Additional Guidelines:\n",
    "- Entity names should be quoted from the conversation, so the output definitions can be referenced back to the conversation.\n",
    "- For the search keyword, use complete, official and context relevant keyword(s) to search for that entity. You might need to autocomplete entity names or use their official names or add additional context keywords to help with searchability, especially if the entity is ambiguous or has multiple meanings. For rare words, include \"definition\" in the search keyword.\n",
    "- Definitions should use simple language to be easily understood.\n",
    "- Select entities whose definitions you are very confident about, otherwise skip them.\n",
    "- Multiple entities can be detected from one phrase, for example, \"The Lugubrious Game\" can be defined as a painting, and the rare word \"lugubrious\" is also worth defining.\n",
    "- Limit results to 3 entities, prioritize rarity.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: If the conversation talks about \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer\", but the entity name would be \"Balmer\" because that is the name quoted from the conversation.\n",
    "    - Replacing unofficial name example: If the conversation talks about \"Clay Institute\", the keyword is \"Clay Mathematics Institute\" since that is the official name, but the entity name would be \"Clay Institute\" because that is the name quoted from the conversation.\n",
    "    - Adding context example: If the conversation talks about \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything (concept)\", because there is a popular movie with the same name. \n",
    "\n",
    "## Recent Definitions:\n",
    "These have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"Moore's Law\", definition: \"Computing power doubles every ~2 yrs\", ekg_search_key: \"Moore's Law\" }}, {{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", ekg_search_key: \"Pareto Principle\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If no relevant entities are identified, output empty arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    pass\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    ekg_search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on Google Enterprise Knowledge Graph\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes, you can find it, okay? If you had this algorithm in your hands, okay? You could ask your computer, you know, I mean, P versus NP is one of these seven problems that carries this million dollar prize from the Clay Foundation. But what I like to say, the way that we can see that P versus NP is the biggest of all of these questions is that if you had this fast algorithm, then you could solve all seven of them, okay? You just ask your computer, you know, is there a short proof of the Riemann hypothesis, right? You know, that a machine could, in a language where a machine could verify it,\n",
      " and provided that such a proof exists, then your computer finds it in a short amount of time without having to do a brute force search, okay? So, I mean, those are the stakes of what we're talking about. But I hope that also helps to give your listeners some intuition of why I and most of my colleagues would put our money on P not equaling NP. Is it possible, I apologize this is a really dumb question, but is it possible to, we should go to the gallery to look at The Lugubrious Game, maybe that will help us relax\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"P versus NP\",\n",
      "      \"definition\": \"Major unsolved problem in computer science\",\n",
      "      \"ekg_search_keyword\": \"P versus NP problem\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Clay Foundation\",\n",
      "      \"definition\": \"Institute offering prizes for math problems\",\n",
      "      \"ekg_search_keyword\": \"Clay Mathematics Institute\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"The Lugubrious Game\",\n",
      "      \"definition\": \"Surrealist painting by Salvador Dalí\",\n",
      "      \"ekg_search_keyword\": \"The Lugubrious Game painting\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='P versus NP', definition='Major unsolved problem in computer science', ekg_search_keyword='P versus NP problem'), Entity(name='Clay Foundation', definition='Institute offering prizes for math problems', ekg_search_keyword='Clay Mathematics Institute'), Entity(name='The Lugubrious Game', definition='Surrealist painting by Salvador Dalí', ekg_search_keyword='The Lugubrious Game painting')]\n"
     ]
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"yes, you can find it, okay? If you had this algorithm in your hands, okay? You could ask your computer, you know, I mean, P versus NP is one of these seven problems that carries this million dollar prize from the Clay Foundation. But what I like to say, the way that we can see that P versus NP is the biggest of all of these questions is that if you had this fast algorithm, then you could solve all seven of them, okay? You just ask your computer, you know, is there a short proof of the Riemann hypothesis, right? You know, that a machine could, in a language where a machine could verify it,\n",
    " and provided that such a proof exists, then your computer finds it in a short amount of time without having to do a brute force search, okay? So, I mean, those are the stakes of what we're talking about. But I hope that also helps to give your listeners some intuition of why I and most of my colleagues would put our money on P not equaling NP. Is it possible, I apologize this is a really dumb question, but is it possible to, we should go to the gallery to look at The Lugubrious Game, maybe that will help us relax\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
