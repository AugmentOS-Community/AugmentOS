{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_158_large...\n",
      "Processing episode_255_large...\n",
      "Processing episode_108_large...\n",
      "Processing episode_145_large...\n",
      "Processing episode_269_large...\n",
      "Processing episode_276_large...\n",
      "Processing episode_251_large...\n",
      "Processing episode_167_large...\n",
      "Processing episode_270_large...\n",
      "Processing episode_095_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Won't selection result in a few pockets of interesting complexities? I mean, yeah, if we ran Earth over again, over and over and over, you're saying it's going to come up with, there's not going to be elephants every time? Yeah, I don't think so. I think that there will be similarities., And I think we don't know enough about how selection globally works. But it might be that the emergence of elephants was wired into the history of Earth in some way, like the gravitational force, how evolution was going, Cambrian explosions, blah, blah, blah, the emergence of mammals. But I just don't know enough about the contingency,, the variability. All I do know is you count the number of bits of information required to make an elephant and think about the causal chain that provide the lineage of elephants going all the way back to Luca, there's a huge scope for divergence. Yeah, but just like you said, with chemistry and selection,, the things that result in self replicating chemistry and self replicating organisms, those are extremely unlikely, as you're saying. But once they're successful, they multiply. So it might be a tiny subset of all things, that are possible in the universe, chemically speaking, it might be a very tiny subset is actually successful at creating elephants. Or elephant like or slash human like creatures. Well, there's two different questions here. The first one, if we were to reset Earth and to start again. The different phases, sorry to keep interrupting. Yeah, no, if we restart Earth and start again, say we could go back to the beginning and do the experiment or have a number of Earths,\\n how similar would biology be? I would say that there would be broad similarities, but the emergence of mammals is not a given unless we're gonna throw an asteroid at each planet at each time and try and faithfully reproduce what happened. Then there's the other thing about when you go to another Earth like planet elsewhere, maybe there's a different ratio, particular elements,\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=20)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective: \n",
    "Identify \"Rare Entities\" in a conversation transcript. These include rare words, phrases, jargons, adages, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. We are using a really lousy transcribing service, so words are often mispelt, but you can autocorrect and piece together implied entities that are described in the conversation context but not explicitly mentioned, your vast knowledge base to derive the \"Rare Entity\" originally mentioned by the user. If you feel the need to search for the entity, then it is most likely mistranscribed.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance:\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose terms with non-obvious meanings, such as \"Butterfly Effect\" and not \"Electric Car\".\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Searchability: Likely to have a specific and valid reference source: Wikipedia page, dictionary entry etc.\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array:\n",
    "entities: [{{ name: string, definition: string, ekg_search_keyword: string }}], where definition is concise (< 12 words), and ekg_search_keyword as the best search keyword for the Google Knowledge Graph.  \n",
    "\n",
    "## Additional Guidelines:\n",
    "- Entity names should be quoted from the conversation, so the output definitions can be referenced back to the conversation, unless they are transcribed wrongly, then use the official name.\n",
    "- For the search keyword, use complete, official and context relevant keyword(s) to search for that entity. You might need to autocomplete entity names or use their official names or add additional context keywords to help with searchability, especially if the entity is ambiguous or has multiple meanings. For rare words, include \"definition\" in the search keyword.\n",
    "- Definitions should use simple language to be easily understood.\n",
    "- Select entities whose definitions you are very confident about, otherwise skip them.\n",
    "- Multiple entities can be detected from one phrase, for example, \"The Lugubrious Game\" can be defined as a painting, and the rare word \"lugubrious\" is also worth defining.\n",
    "- Limit results to 4 or less entities, prioritize rarity.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: If the conversation talks about \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer\", but the entity name would be \"Balmer\" because that is the name quoted from the conversation.\n",
    "    - Replacing unofficial name example: If the conversation talks about \"Clay Institute\", the keyword is \"Clay Mathematics Institute\" since that is the official name, but the entity name would be \"Clay Institute\" because that is the name quoted from the conversation.\n",
    "    - Adding context example: If the conversation talks about \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything (concept)\", because there is a popular movie with the same name. \n",
    "\n",
    "## Recent Definitions:\n",
    "These have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"Moore's Law\", definition: \"Computing power doubles every ~2 yrs\", ekg_search_key: \"Moore's Law\" }}, {{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", ekg_search_key: \"Pareto Principle\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If no relevant entities are identified, output empty arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    \n",
    "    return proactive_rare_word_agent_response\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    ekg_search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on Google Enterprise Knowledge Graph\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions()\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in NLP\",\n",
      "      \"ekg_search_keyword\": \"Hugging Face\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research laboratory\",\n",
      "      \"ekg_search_keyword\": \"OpenAI\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"Large language machine learning model\",\n",
      "      \"ekg_search_keyword\": \"Falcon LLM model\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in NLP', ekg_search_keyword='Hugging Face'), Entity(name='OpenAI', definition='AI research laboratory', ekg_search_keyword='OpenAI'), Entity(name='Falcon LLM model', definition='Large language machine learning model', ekg_search_keyword='Falcon LLM model')]\n"
     ]
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tool\n",
    "EKG is unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Literal\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "k: int = 3\n",
    "gl: str = \"us\"\n",
    "hl: str = \"en\"\n",
    "tbs = None\n",
    "num_sentences = 7\n",
    "serper_api_key=os.environ.get(\"SERPER_API_KEY\")\n",
    "search_type: Literal[\"news\", \"search\", \"places\", \"images\"] = \"search\"\n",
    "result_key_for_type = {\n",
    "        \"news\": \"news\",\n",
    "        \"places\": \"places\",\n",
    "        \"images\": \"images\",\n",
    "        \"search\": \"organic\",\n",
    "    }\n",
    "\n",
    "async def serper_search_async(\n",
    "    search_term: str, search_type: str = \"search\", **kwargs: Any\n",
    ") -> dict:\n",
    "    headers = {\n",
    "        \"X-API-KEY\": serper_api_key or \"\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": search_term,\n",
    "        **{key: value for key, value in kwargs.items() if value is not None},\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"https://google.serper.dev/{search_type}\", headers=headers, json=params) as response:\n",
    "            response.raise_for_status()\n",
    "            search_results = await response.json()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "async def parse_snippets_async(results: dict, scrape_pages: bool = False, summarize_pages: bool = True, num_sentences: int = 3) -> List[str]:\n",
    "    snippets = []\n",
    "    if results.get(\"answerBox\"):\n",
    "        answer_box = results.get(\"answerBox\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            snippets.append(f\"The answer is {answer_box.get('answer')}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippet')}\")\n",
    "        elif answer_box.get(\"snippetHighlighted\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippetHighlighted')}\")\n",
    "\n",
    "    if results.get(\"knowledgeGraph\"):\n",
    "        kg = results.get(\"knowledgeGraph\", {})\n",
    "        title = kg.get(\"title\")\n",
    "        entity_type = kg.get(\"type\")\n",
    "        if entity_type:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {entity_type}.\")\n",
    "        description = kg.get(\"description\")\n",
    "        if description:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {description}.\")\n",
    "        for attribute, value in kg.get(\"attributes\", {}).items():\n",
    "            snippets.append(f\"Knowledge Graph Results: {title} {attribute}: {value}.\")\n",
    "\n",
    "    if scrape_pages:\n",
    "        tasks = []\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            task = asyncio.create_task(scrape_page_async(result[\"link\"], summarize_page=summarize_pages, num_sentences=num_sentences))\n",
    "            tasks.append(task)\n",
    "        summarized_pages = await asyncio.gather(*tasks)\n",
    "        for i, page in enumerate(summarized_pages):\n",
    "            result = results[result_key_for_type[search_type]][i]\n",
    "            if page:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\nSummarized Page: {page}\")\n",
    "            else:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "    else:\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "\n",
    "    if len(snippets) == 0:\n",
    "        return [\"No good Google Search Result was found\"]\n",
    "    return snippets\n",
    "\n",
    "def extract_entity_url_and_image(results: dict):\n",
    "    print(results)\n",
    "    res = {}\n",
    "    if results.get(\"knowledgeGraph\"):\n",
    "        res[\"url\"] = results.get(\"knowledgeGraph\", {}).get(\"descriptionLink\")\n",
    "        res[\"imageUrl\"] = results.get(\"knowledgeGraph\", {}).get(\"imageUrl\")\n",
    "\n",
    "    for result in results[result_key_for_type[search_type]][:k]:\n",
    "        if result.get(\"link\"):\n",
    "            res[\"url\"] = result.get(\"link\")\n",
    "    \n",
    "    return res\n",
    "\n",
    "async def search_url_for_entity_async(query: str):\n",
    "    results = await serper_search_async(\n",
    "        search_term=query,\n",
    "        gl=gl,\n",
    "        hl=hl,\n",
    "        num=k,\n",
    "        tbs=tbs,\n",
    "        search_type=search_type,\n",
    "    )\n",
    "    return extract_entity_url_and_image(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'searchParameters': {'q': 'ChatGpt', 'gl': 'us', 'hl': 'en', 'num': 3, 'type': 'search', 'engine': 'google'}, 'knowledgeGraph': {'title': 'ChatGPT', 'type': 'Computer program', 'imageUrl': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSE0_kvfFhLnJU0KZHDXOdED-y5VUeuVi9TmqBcKSg&s=0', 'description': 'ChatGPT is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, that enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language.', 'descriptionSource': 'Wikipedia', 'descriptionLink': 'https://en.wikipedia.org/wiki/ChatGPT', 'attributes': {'Initial release date': 'November 30, 2022', 'Developer(s)': 'OpenAI', 'Engine': 'GPT-3.5 (free and paid); GPT-4 (paid only)', 'License': 'Proprietary service', 'Platform': 'Cloud computing platforms', 'Stable release': 'November 21, 2023; 13 days ago', 'Written in': 'Python'}}, 'organic': [{'title': 'ChatGPT', 'link': 'https://chat.openai.com/', 'snippet': 'ChatGPT is a free-to-use AI system. Use it for engaging conversations, gain insights, automate tasks, and witness the future of AI, all in one place.', 'position': 1}, {'title': 'Introducing ChatGPT - OpenAI', 'link': 'https://openai.com/blog/chatgpt', 'snippet': 'ChatGPT is a sibling model to InstructGPT, which is trained to follow an instruction in a prompt and provide a detailed response.', 'date': 'Nov 30, 2022', 'position': 2}, {'title': 'OpenAI', 'link': 'https://openai.com/', 'snippet': 'DALL·E 3 is now available in ChatGPT Plus and Enterprise. Oct 19, 2023October 19, 2023 · ChatGPT Can Now See Hear And Speak. ChatGPT can now see, hear, and ...', 'position': 3}], 'relatedSearches': [{'query': 'How to use ChatGPT'}, {'query': 'ChatGPT login with Google'}, {'query': 'ChatGPT free online'}, {'query': 'OpenAI login'}, {'query': 'chat.openai.com login'}, {'query': 'ChatGPT alternative'}, {'query': 'Chat GPT login free'}, {'query': 'Auto ChatGPT'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://openai.com/',\n",
       " 'imageUrl': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSE0_kvfFhLnJU0KZHDXOdED-y5VUeuVi9TmqBcKSg&s=0'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await search_url_for_entity_async(\"ChatGpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in NLP models\",\n",
      "      \"ekg_search_keyword\": \"Hugging Face\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research lab, created GPT models\",\n",
      "      \"ekg_search_keyword\": \"OpenAI\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"Large language model for AI applications\",\n",
      "      \"ekg_search_keyword\": \"Falcon LLM model\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in NLP models', ekg_search_keyword='Hugging Face'), Entity(name='OpenAI', definition='AI research lab, created GPT models', ekg_search_keyword='OpenAI'), Entity(name='Falcon LLM model', definition='Large language model for AI applications', ekg_search_keyword='Falcon LLM model')]\n",
      "entities=[Entity(name='Hugging Face', definition='AI company specializing in NLP models', ekg_search_keyword='Hugging Face'), Entity(name='OpenAI', definition='AI research lab, created GPT models', ekg_search_keyword='OpenAI'), Entity(name='Falcon LLM model', definition='Large language model for AI applications', ekg_search_keyword='Falcon LLM model')]\n",
      "Hugging Face\n",
      "{'url': 'https://github.com/huggingface', 'imageUrl': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQe95xXWC9oVNc8Kss7TwKOWq5d4-Zg2s_kMxYrBVfMqOKAPR3DZfRCWX4&s=0'}\n",
      "OpenAI\n",
      "{'url': 'https://en.wikipedia.org/wiki/OpenAI', 'imageUrl': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT8pfsL6cbfea_zrYx9SMix7ckz4aRhubZuHIXLmMSmvfDIZXxYAr6Bj88&s=0'}\n",
      "Falcon LLM model\n",
      "{'url': 'https://falconllm.tii.ae/'}\n"
     ]
    }
   ],
   "source": [
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falkon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "print(res)\n",
    "for entities in res.entities:\n",
    "    print(entities.name)\n",
    "    print(await search_url_for_entity_async(entities.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
