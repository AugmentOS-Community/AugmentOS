{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_069_large...\n",
      "Processing episode_305_large...\n",
      "Processing episode_166_large...\n",
      "Processing episode_023_large...\n",
      "Processing episode_095_large...\n",
      "Processing episode_238_large...\n",
      "Processing episode_058_large...\n",
      "Processing episode_239_large...\n",
      "Processing episode_253_large...\n",
      "Processing episode_006_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The art of running such a lab is that there are strategic priorities for the company. And there are areas where we do want to invest and pressing problems. And so it's a little bit of a trickle down and filter up meets in the middle. And so you don't tell people you have to do X, but you say X would be particularly appreciated this year. And then people reinterpret, X through the filter of things they want to do and they're interested in. And miraculously, it usually comes together very well. One thing that really helps is Adobe has a really broad portfolio of products. So if we have a good idea, there's usually a product team that is intrigued or interested. So it means we don't have to qualify things too much ahead of time., Once in a while, the product teams sponsor extra intern, because they have a particular problem that they really care about, in which case it's a little bit more, we really need one of these. And then we sort of say, great, I get an extra intern, we find an intern who thinks that's a great problem. But that's not the typical model. That's sort of the icing on the cake as far as the budget is concerned. And all of the above end up being important. It's really hard to predict, at the beginning of the summer, which we all have high hopes of all of the intern projects, but ultimately, some of them pay off and some of them sort of are a nice paper, but don't turn into a feature. Others turn out not to be as novel as we thought, but they'd be a great feature, but not a paper. And then others, we make a little bit of progress and we realize how much we don't know. And maybe we revisit that problem several years in a row until it,, finally we have a breakthrough and then it becomes more on track to impact a product. Jumping back to a big overall view of Adobe research, what are you looking forward to in 2019 and beyond? What is, you mentioned there's a giant suite of products, a giant suite of ideas, new interns, a large team of researchers.\\n What do you think the future holds? In terms of the technological breakthroughs? Technological breakthroughs, especially ones that will make it into product, will get to impact the world. So I think the creative or the analytics assistants that we talked about where they're constantly trying to figure out what you're trying to do and how can they be helpful and make useful suggestions is a really hot topic. And it's very unpredictable as to when\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=20)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective\n",
    "Your role is to identify and define \"Rare Entities\" in a conversation transcript. Types of \"Rare Entities\" include rare words, phrases, jargons, adages, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. You can also intelligently detect entities that are described in the conversation but not explicitly mentioned.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose terms with non-obvious meanings, such as \"Butterfly Effect\" but not \"Electric Car\".\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Existance: Don't select entities if you have no knowledge of them\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array (ONLY OUTPUT THIS) of the entities you identified using the following template: `[{{ name: string, definition: string, search_keyword: string }}]`\n",
    "\n",
    "- name is the entity name shown to the user, if it is mistranscribed, autocorrect it, otherwise use the name quoted from the conversation\n",
    "- definition is concise (< 12 words)\n",
    "- search_keyword as the best Internet search keywords to find the entity, add entity type defined above for better searchability\n",
    "- it's OK to output an empty array - most of the time, the array will be empty, only include items if the fit all the requirements\n",
    "\n",
    "## Additional Guidelines:\n",
    "- Do not define entities you yourself are not familiar with, you can try to piece together the implied entity, but if you are not 90% confident, skip it.\n",
    "- For the search keyword, use complete, official and context relevant keyword(s) to search for that entity. You might need to autocomplete entity names or use their official names or add additional context keywords (like the type of entity) to help with searchability, especially if the entity is ambiguous or has multiple meanings. Additionally, for rare words, add \"definition\" to the search keyword.\n",
    "- Definitions should use simple language to be easily understood.\n",
    "- Select entities whose definitions you are very confident about, otherwise skip them.\n",
    "- Multiple entities can be detected from one phrase, for example, \"The Lugubrious Game\" can be defined as a painting (iff the entire term \"the lugubrious game\" is mentioned), and the rare word \"lugubrious\" is also worth defining.\n",
    "- Limit results to {number_of_definitions} entities, prioritize rarity.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: If the conversation talks about \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer + person\", and the name would be \"Steve Balmer\" because it is complete.\n",
    "    - Replacing unofficial name example: If the conversation talks about \"Clay Institute\", the keyword is \"Clay Mathematics Institute + organization\" since that is the official name, but the entity name would be \"Clay Institute\" because that is the name quoted from the conversation.\n",
    "    - Adding context example: If the conversation talks about \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything + concept\", because there is a popular movie with the same name. \n",
    "    - Inferring transcript errors example: If the conversation mentions \"Coleman Sachs\" in the context of finance, you can infer it was supposed to be \"Goldman Sachs\", so you autocorrect and define it as \"Goldman Sachs\" and give its definition.\n",
    "\n",
    "## Recent Definitions:\n",
    "These have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", search_keyword: \"80/20 Rule + concept\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If no relevant entities are identified, output empty arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    \n",
    "    return proactive_rare_word_agent_response\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on the Internet\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions(),\n",
    "            \"number_of_definitions\": 3,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException as e:\n",
    "        print(\"Error parsing output\" , e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in natural language processing\",\n",
      "      \"search_keyword\": \"Hugging Face + AI company\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research lab, creators of GPT models\",\n",
      "      \"search_keyword\": \"OpenAI + AI research lab\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"A large language model for AI applications\",\n",
      "      \"search_keyword\": \"Falcon LLM model + AI\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + AI company'), Entity(name='OpenAI', definition='AI research lab, creators of GPT models', search_keyword='OpenAI + AI research lab'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + AI')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + AI company'), Entity(name='OpenAI', definition='AI research lab, creators of GPT models', search_keyword='OpenAI + AI research lab'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + AI')])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tool\n",
    "EKG is unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Literal\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "k: int = 3\n",
    "gl: str = \"us\"\n",
    "hl: str = \"en\"\n",
    "tbs = None\n",
    "num_sentences = 7\n",
    "serper_api_key=os.environ.get(\"SERPER_API_KEY\")\n",
    "search_type: Literal[\"news\", \"search\", \"places\", \"images\"] = \"images\"\n",
    "result_key_for_type = {\n",
    "        \"news\": \"news\",\n",
    "        \"places\": \"places\",\n",
    "        \"images\": \"images\",\n",
    "        \"search\": \"organic\",\n",
    "    }\n",
    "\n",
    "async def serper_search_async(\n",
    "    search_term: str, search_type: str = \"search\", **kwargs: Any\n",
    ") -> dict:\n",
    "    headers = {\n",
    "        \"X-API-KEY\": serper_api_key or \"\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": search_term,\n",
    "        **{key: value for key, value in kwargs.items() if value is not None},\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"https://google.serper.dev/{search_type}\", headers=headers, json=params) as response:\n",
    "            response.raise_for_status()\n",
    "            search_results = await response.json()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "async def parse_snippets_async(results: dict, scrape_pages: bool = False, summarize_pages: bool = True, num_sentences: int = 3) -> List[str]:\n",
    "    snippets = []\n",
    "    if results.get(\"answerBox\"):\n",
    "        answer_box = results.get(\"answerBox\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            snippets.append(f\"The answer is {answer_box.get('answer')}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippet')}\")\n",
    "        elif answer_box.get(\"snippetHighlighted\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippetHighlighted')}\")\n",
    "\n",
    "    if results.get(\"knowledgeGraph\"):\n",
    "        kg = results.get(\"knowledgeGraph\", {})\n",
    "        title = kg.get(\"title\")\n",
    "        entity_type = kg.get(\"type\")\n",
    "        if entity_type:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {entity_type}.\")\n",
    "        description = kg.get(\"description\")\n",
    "        if description:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {description}.\")\n",
    "        for attribute, value in kg.get(\"attributes\", {}).items():\n",
    "            snippets.append(f\"Knowledge Graph Results: {title} {attribute}: {value}.\")\n",
    "\n",
    "    if scrape_pages:\n",
    "        tasks = []\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            task = asyncio.create_task(scrape_page_async(result[\"link\"], summarize_page=summarize_pages, num_sentences=num_sentences))\n",
    "            tasks.append(task)\n",
    "        summarized_pages = await asyncio.gather(*tasks)\n",
    "        for i, page in enumerate(summarized_pages):\n",
    "            result = results[result_key_for_type[search_type]][i]\n",
    "            if page:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\nSummarized Page: {page}\")\n",
    "            else:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "    else:\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "\n",
    "    if len(snippets) == 0:\n",
    "        return [\"No good Google Search Result was found\"]\n",
    "    return snippets\n",
    "\n",
    "import requests\n",
    "\n",
    "def can_embed_url(url: str):\n",
    "    response = requests.head(url)\n",
    "\n",
    "    # Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "    x_frame_options = response.headers.get('X-Frame-Options')\n",
    "    csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "    return x_frame_options or ('frame-ancestors' in csp if csp else False)\n",
    "\n",
    "def extract_entity_url_and_image(search_results: dict, image_results: dict):\n",
    "    # Only get the first top url and image_url\n",
    "    res = {}\n",
    "    if search_results.get(\"knowledgeGraph\"):\n",
    "        result = search_results.get(\"knowledgeGraph\", {})\n",
    "        if result.get(\"descriptionSource\") == \"Wikipedia\":\n",
    "            ref_url = result.get(\"descriptionLink\")\n",
    "            res[\"url\"] = ref_url\n",
    "\n",
    "    for result in search_results[result_key_for_type[\"search\"]][:k]:\n",
    "        if \"url\" not in res and result.get(\"link\") and can_embed_url(result.get(\"link\")):\n",
    "            res[\"url\"] = result.get(\"link\")\n",
    "            break\n",
    "\n",
    "    if image_results is None:\n",
    "        return res\n",
    "    \n",
    "    for result in image_results[result_key_for_type[\"images\"]][:k]:\n",
    "        if \"image_url\" not in res and result.get(\"imageUrl\"):\n",
    "            res[\"image_url\"] = result.get(\"imageUrl\")\n",
    "            break\n",
    "\n",
    "    return res\n",
    "\n",
    "async def search_url_for_entity_async(query: str):\n",
    "    search_results = await serper_search_async(\n",
    "        search_term=query,\n",
    "        gl=gl,\n",
    "        hl=hl,\n",
    "        num=k,\n",
    "        tbs=tbs,\n",
    "        search_type=\"search\",\n",
    "    )\n",
    "\n",
    "    image_results = None if \"definition\" in query else await serper_search_async(\n",
    "            search_term=query,\n",
    "            gl=gl,\n",
    "            hl=hl,\n",
    "            num=k,\n",
    "            tbs=tbs,\n",
    "            search_type=\"images\",\n",
    "        )\n",
    "    \n",
    "    return extract_entity_url_and_image(search_results, image_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://en.wikipedia.org/wiki/ChatGPT',\n",
       " 'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/ChatGPT_logo.svg/1200px-ChatGPT_logo.svg.png'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await search_url_for_entity_async(\"ChatGpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And sometimes they're in a network. So you imagine them connected with network links and a dynamic network, those can change, right? So I was talking to you, but now I can't talk to you anymore. Now I'm connected to a person over here. It's a really hard environment mathematically speaking. And there's a lot of really strong lower bounds, which you could imagine if the network can change all the time and a bad guy is doing it, it's like hard to do things well., So there's an algorithm running on every single node in the network. Yeah. And then you're trying to say something of any kind that makes any kind of definitive sense about the performance of that algorithm. Yeah, so I just submitted a new paper on this a couple of weeks ago. And we were looking at a very simple problem. There's some messages in the network. We want everyone to get them. If the network doesn't change,, you can do this pretty well. You can pipeline them. There's some basic algorithms that work really well. If the network can change every round, there's these lower bounds that says, it takes a really long time. There's a way that no matter what algorithm you come up with, there's a way the network can change in such a way that just really slows down your progress basically, right? So smooth analysis there says, yeah, but that seems like you'd have really bad luck, if your network was changing exactly in the right way that you needed to screw your algorithm. So we said, what if we randomly just add or remove a couple of edges in every round? So the adversary is trying to choose the worst possible network. And we're just tweaking it a little bit. And in that case, this is a new paper. I mean, it's a blinded submission, so maybe I shouldn't, it's not, whatever. We basically showed., An anonymous friend of yours submitted a paper. An anonymous friend of mine, yeah, whose paper should be accepted. Showed that even just adding like one random edge per round, and here's the cool thing about it, the simplest possible solution to this problem blows away that lower bound and does really well. So that's like a very fragile lower bound because we're like, it's almost impossible\n",
      " to actually keep things slow. I wonder how many lower bounds you can smash open with this kind of analysis and show that they're fragile. It's my interest, yeah. Because in distributed algorithms, there's a ton of really famous strong lower bounds, but things have to go wrong, really, really wrong for these lower bound arguments to work.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"dynamic network\",\n",
      "      \"definition\": \"Network whose connections change over time\",\n",
      "      \"search_keyword\": \"dynamic network + computer science\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"lower bounds\",\n",
      "      \"definition\": \"Minimum performance measure in algorithms\",\n",
      "      \"search_keyword\": \"lower bounds + algorithms\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"smooth analysis\",\n",
      "      \"definition\": \"Framework for analyzing algorithms' performance\",\n",
      "      \"search_keyword\": \"smooth analysis + algorithms\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='dynamic network', definition='Network whose connections change over time', search_keyword='dynamic network + computer science'), Entity(name='lower bounds', definition='Minimum performance measure in algorithms', search_keyword='lower bounds + algorithms'), Entity(name='smooth analysis', definition=\"Framework for analyzing algorithms' performance\", search_keyword='smooth analysis + algorithms')]\n",
      "entities=[Entity(name='dynamic network', definition='Network whose connections change over time', search_keyword='dynamic network + computer science'), Entity(name='lower bounds', definition='Minimum performance measure in algorithms', search_keyword='lower bounds + algorithms'), Entity(name='smooth analysis', definition=\"Framework for analyzing algorithms' performance\", search_keyword='smooth analysis + algorithms')]\n",
      "dynamic network + computer science\n",
      "{'url': 'https://www.sciencedirect.com/topics/computer-science/dynamic-network', 'image_url': 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/DynamicNetworkAnalysisExample.jpg/340px-DynamicNetworkAnalysisExample.jpg'}\n",
      "lower bounds + algorithms\n",
      "{'url': 'https://www.geeksforgeeks.org/lower-and-upper-bound-theory/', 'image_url': 'https://i.ytimg.com/vi/PNPKUcX940o/maxresdefault.jpg'}\n",
      "smooth analysis + algorithms\n",
      "{'url': 'https://en.wikipedia.org/wiki/Smoothed_analysis', 'image_url': 'https://i.ytimg.com/vi/nRnn96xf_MI/hqdefault.jpg'}\n"
     ]
    }
   ],
   "source": [
    "test_transcript = generate_test_input()\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "print(res)\n",
    "for entities in res.entities:\n",
    "    print(entities.search_keyword)\n",
    "    print(await search_url_for_entity_async(entities.search_keyword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just go for the best intuitive way that works the best\n",
    "\n",
    "Pipeline\n",
    "1. Check if page can be embed\n",
    "2. Check if url is accurate for definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if page can be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page can be embedded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the page you want to check\n",
    "url = 'https://www.labellerr.com/blog/what-are-adversarial-attacks-in-machine-learning-and-how-can-you-prevent-them/'\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.head(url)\n",
    "\n",
    "# Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "x_frame_options = response.headers.get('X-Frame-Options')\n",
    "csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "if x_frame_options or ('frame-ancestors' in csp if csp else False):\n",
    "    print(\"The page cannot be embedded.\")\n",
    "else:\n",
    "    print(\"The page can be embedded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
