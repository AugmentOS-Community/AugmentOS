{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definer Experiments\n",
    "Previous notebook was getting quite full, here is a new notebooks for the Definer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all imports here to be efficient\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import OutputParserException\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing episode_205_large...\n",
      "Processing episode_196_large...\n",
      "Processing episode_141_large...\n",
      "Processing episode_269_large...\n",
      "Processing episode_035_large...\n",
      "Processing episode_112_large...\n",
      "Processing episode_017_large...\n",
      "Processing episode_136_large...\n",
      "Processing episode_254_large...\n",
      "Processing episode_192_large...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" breaks you know stopping breaks you got to stay on top of nutrition that's the other big thing too i'm you know probably eating like anywhere between 10 to 15 000 calories a day which is you know i can probably count on my hand a couple of occasions where i've eaten that much in my life so now i got to do that for six plus weeks in a row and you don't want any having a stomach problem i'm, trying to try to minimize the amount of stomach problems so you would you estimate about 12 to 13 to 14 hours of running every day yeah that's probably like from from the first step to the last step it'll probably be somewhere around like say 14 hours 13 hours or something like that would be a pretty good estimate and then getting rest and so and then minimizing the risk of injury, which could be as small as like like literally uneven surfaces resulting to like stepping the wrong way i mean that's going to be a lot of steps yeah yeah uh huh so the probability of injury are you worried about that kind of stuff is can you strengthen the ankles or those kinds of things that prevent yeah possibility of injury and that's that's where i'm putting a lot of my focus in is, i think like just being running fit is going to be like generally speaking is going to be important i'm going to i think just from a lifetime of running is going to be a huge advantage a lot of these like kind of like mechanical movements are going to be very established it's just going to be about can i tolerate that volume of it there i think that i'm doing more, strength work i think this is something where it's like you know maybe adding five pounds of lower body muscle is going to be an advantage versus a disadvantage when you're looking at power weight ratio because i just don't really don't i don't i never need to be running a 648 mile for this adventure um and i so i'm looking at that i'm doing a lot more of that stuff focusing\\n on that the training is changing a fair bit where it's more polarizing versus kind of being i mean i've always had some polarization in my training but this is even to an extreme where like i'm going to do some simulations where uh you know i go out and do two or three days where i target the exact thing i will be doing on the transcon you were on instagram posting about\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Way to generate a random test input using transcripts from Lex Fridman's podcast\n",
    "# Make sure you have the transcripts downloaded in the folder lex_whisper_transcripts\n",
    "\n",
    "import test_on_lex\n",
    "\n",
    "transcripts = test_on_lex.load_lex_transcripts(random_n=10, transcript_folder=\"./lex_whisper_transcripts/\", chunk_time_seconds=15)\n",
    "\n",
    "import random\n",
    "def generate_test_input():\n",
    "    idx = random.randint(0, 10)\n",
    "    key = list(transcripts.keys())[idx]\n",
    "    transcript = transcripts[key]\n",
    "    trans_idx = random.randint(10, len(transcript)-10)\n",
    "    latest = transcript[trans_idx:trans_idx+7]\n",
    "    prev_transcripts, curr_transcripts = str.join(\",\", list(latest[0:5])), latest[5]\n",
    "    return prev_transcripts + \"\\n\" + curr_transcripts\n",
    "\n",
    "generate_test_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list_data(list_data: list):\n",
    "    return \"\\n\".join([f\"{i+1}. {example}\" for i, example in enumerate(list_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proactive_rare_word_agent_prompt_blueprint = \"\"\"\n",
    "# Objective\n",
    "Your role is to identify and define \"Rare Entities\" in a transcript. Types of \"Rare Entities\" include rare words, jargons, adages, concepts, people, places, organizations, events etc that are not well known to the average high schooler, in accordance to current trends. You can also intelligently detect entities that are described in the conversation but not explicitly mentioned.\n",
    "\n",
    "# Criteria for Rare Entities in order of importance\n",
    "1. Rarity: Select entities that are unlikely for an average high schooler to know. Well known entities are like Fortune 500 organizations, worldwide-known events, popular locations, and entities popularized by recent news or events such as \"COVID-19\", \"Bitcoin\", or \"Generative AI\".\n",
    "2. Utility: Definition should help a user understand the conversation better and achieve their goals.\n",
    "3. No Redundancy: Exclude definitions if already defined in the conversation.\n",
    "4. Complexity: Choose phrases with non-obvious meanings, such that their meaning cannot be derived from simple words within the entity name, such as \"Butterfly Effect\" which has a totally different meaning from its base words, but not \"Electric Car\" nor \"Lane Keeping System\" as they're easily derived.\n",
    "5. Definability: Must be clearly and succinctly definable in under 10 words.\n",
    "6. Existance: Don't select entities if you have no knowledge of them\n",
    "\n",
    "# Conversation Transcript:\n",
    "<Transcript start>{conversation_context}<Transcript end>\n",
    "\n",
    "# Output Guidelines:\n",
    "Output an array (ONLY OUTPUT THIS) of the entities you identified using the following template: `[{{ name: string, definition: string, search_keyword: string }}]`\n",
    "\n",
    "- name is the entity name shown to the user, if the name is mistranscribed, autocorrect it into the most well known form with proper spelling, capitalization and punctuation\n",
    "- definition is concise (< 12 words)\n",
    "- search_keyword as the best specific Internet search keywords to search for the entity, add entity type defined above for better searchability\n",
    "- it's OK to output an empty array - most of the time, the array will be empty, only include items if the fit all the requirements\n",
    "\n",
    "## Additional Guidelines:\n",
    "- Only select nouns, not verbs or adjectives.\n",
    "- Select entities iff they have an entry in an encyclopedia, wikipedia, dictionary, or other reference material.\n",
    "- Do not define entities you yourself are unfamiliar with, you can try to piece together the implied entity only if you are 99% confident.\n",
    "- For the search keyword, use complete, official and context relevant keyword(s) to search for that entity. You might need to autocorrect entity names or use their official names or add additional context keywords (like the type of entity) to help with searchability, especially if the entity is ambiguous or has multiple meanings. Additionally, for rare words, add \"definition\" to the search keyword.\n",
    "- Definitions should use simple language to be easily understood.\n",
    "- Multiple entities can be detected from one phrase, for example, \"The Lugubrious Game\" can be defined as a painting (iff the entire term \"the lugubrious game\" is mentioned), and the rare word \"lugubrious\" is also a candidate to define.\n",
    "- Limit results to {number_of_definitions} entities, prioritize rarity and utility.\n",
    "- Examples:\n",
    "    - Completing incomplete name example: Conversation mentions \"Balmer\" and \"Microsoft\", the keyword is \"Steve Balmer + person\", and the name would be \"Steve Balmer\" because it is complete.\n",
    "    - Replacing unofficial name example: Conversation mentions \"Clay Institute\", the keyword is \"Clay Mathematics Institute + organization\", using the official name.\n",
    "    - Add context example: Conversation mentions \"Theory of everything\", the keyword needs context keywords such as \"Theory of everything + concept\", because there is a popular movie with the same name. \n",
    "    - Autocorrect transcript example: Conversation mentions \"Coleman Sachs\" in the context of finance, if you're confident it was supposed to be \"Goldman Sachs\", you autocorrect it and define \"Goldman Sachs\".\n",
    "\n",
    "## Recent Definitions:\n",
    "These have already been defined so don't define them again:\n",
    "{definitions_history}\n",
    "\n",
    "## Example Output:\n",
    "entities: [{{ name: \"80/20 Rule\", definition: \"Productivity concept; Majority of results come from few causes\", search_keyword: \"80/20 Rule + concept\" }}]\n",
    "\n",
    "{format_instructions} \n",
    "If no relevant entities are identified, output empty arrays.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_proactive_rare_word_agent_and_definer(\n",
    "    conversation_context: str, definitions_history: list = []\n",
    "):\n",
    "    # run proactive agent to find out which expert agents we should run\n",
    "    proactive_rare_word_agent_response = run_proactive_rare_word_agent(\n",
    "        conversation_context, definitions_history\n",
    "    )\n",
    "\n",
    "    # do nothing else if proactive meta agent didn't specify an agent to run\n",
    "    if proactive_rare_word_agent_response == []:\n",
    "        return []\n",
    "\n",
    "    # pass words to define to definer agent\n",
    "    print(\"proactive_rare_word_agent_response\", proactive_rare_word_agent_response)\n",
    "    \n",
    "    return proactive_rare_word_agent_response\n",
    "\n",
    "class ProactiveRareWordAgentQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Proactive rare word agent that identifies rare entities in a conversation context\n",
    "    \"\"\"\n",
    "\n",
    "    to_define_list: list = Field(\n",
    "        description=\"the rare entities to define\",\n",
    "    )\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"entity name\",\n",
    "    )\n",
    "    definition: str = Field(\n",
    "        description=\"entity definition\",\n",
    "    )\n",
    "    search_keyword: str = Field(\n",
    "        description=\"keyword to search for entity on the Internet\",\n",
    "    )\n",
    "\n",
    "class ConversationEntities(BaseModel):\n",
    "    entities: List[Entity] = Field(\n",
    "        description=\"list of entities and their definitions\",\n",
    "        default=[]\n",
    "    )\n",
    "\n",
    "proactive_rare_word_agent_query_parser = PydanticOutputParser(\n",
    "    pydantic_object=ConversationEntities\n",
    ")\n",
    "\n",
    "def run_proactive_rare_word_agent(conversation_context: str, definitions_history: list):\n",
    "    # start up GPT4 connection\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=os.environ.get(\"OPEN_AI_API_KEY\"),\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "    )\n",
    "\n",
    "    extract_proactive_rare_word_agent_query_prompt = PromptTemplate(\n",
    "        template=proactive_rare_word_agent_prompt_blueprint,\n",
    "        input_variables=[\n",
    "            \"conversation_context\",\n",
    "            \"definitions_history\",\n",
    "        ],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": proactive_rare_word_agent_query_parser.get_format_instructions(),\n",
    "            \"number_of_definitions\": 3,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if len(definitions_history) > 0:\n",
    "        definitions_history = format_list_data(definitions_history)\n",
    "    else:\n",
    "        definitions_history = \"None\"\n",
    "\n",
    "    proactive_rare_word_agent_query_prompt_string = (\n",
    "        extract_proactive_rare_word_agent_query_prompt.format_prompt(\n",
    "            conversation_context=conversation_context,\n",
    "            definitions_history=definitions_history,\n",
    "        ).to_string()\n",
    "    )\n",
    "\n",
    "    # print(\"Proactive meta agent query prompt string\", proactive_rare_word_agent_query_prompt_string)\n",
    "\n",
    "    response = llm(\n",
    "        [HumanMessage(content=proactive_rare_word_agent_query_prompt_string)]\n",
    "    )\n",
    "\n",
    "    print(response.content)\n",
    "    try:\n",
    "        res = proactive_rare_word_agent_query_parser.parse(\n",
    "            response.content\n",
    "        )\n",
    "        return res\n",
    "    except OutputParserException as e:\n",
    "        print(\"Error parsing output\" , e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hugging Face\",\n",
      "      \"definition\": \"AI company specializing in natural language processing\",\n",
      "      \"search_keyword\": \"Hugging Face + organization\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"OpenAI\",\n",
      "      \"definition\": \"AI research lab focusing on safe artificial general intelligence\",\n",
      "      \"search_keyword\": \"OpenAI + organization\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Falcon LLM model\",\n",
      "      \"definition\": \"A large language model for AI applications\",\n",
      "      \"search_keyword\": \"Falcon LLM model + artificial intelligence\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + organization'), Entity(name='OpenAI', definition='AI research lab focusing on safe artificial general intelligence', search_keyword='OpenAI + organization'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + artificial intelligence')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Hugging Face', definition='AI company specializing in natural language processing', search_keyword='Hugging Face + organization'), Entity(name='OpenAI', definition='AI research lab focusing on safe artificial general intelligence', search_keyword='OpenAI + organization'), Entity(name='Falcon LLM model', definition='A large language model for AI applications', search_keyword='Falcon LLM model + artificial intelligence')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_transcript = generate_test_input()\n",
    "test_transcript = \"\"\"\n",
    "In the realm of artificial intelligence and big data, several key players stand out with their innovative contributions. Hugging Fase, a leader in machine learning models. Another major entity, OpenYI, has revolutionized language models. We now have the largest LLMs ever such as the Falcon LLM model\"\"\"\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search tool\n",
    "EKG is unreliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Literal\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "k: int = 3\n",
    "gl: str = \"us\"\n",
    "hl: str = \"en\"\n",
    "tbs = None\n",
    "num_sentences = 7\n",
    "serper_api_key=os.environ.get(\"SERPER_API_KEY\")\n",
    "search_type: Literal[\"news\", \"search\", \"places\", \"images\"] = \"images\"\n",
    "result_key_for_type = {\n",
    "        \"news\": \"news\",\n",
    "        \"places\": \"places\",\n",
    "        \"images\": \"images\",\n",
    "        \"search\": \"organic\",\n",
    "    }\n",
    "\n",
    "async def serper_search_async(\n",
    "    search_term: str, search_type: str = \"search\", **kwargs: Any\n",
    ") -> dict:\n",
    "    headers = {\n",
    "        \"X-API-KEY\": serper_api_key or \"\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    params = {\n",
    "        \"q\": search_term,\n",
    "        **{key: value for key, value in kwargs.items() if value is not None},\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"https://google.serper.dev/{search_type}\", headers=headers, json=params) as response:\n",
    "            response.raise_for_status()\n",
    "            search_results = await response.json()\n",
    "            return search_results\n",
    "\n",
    "\n",
    "async def parse_snippets_async(results: dict, scrape_pages: bool = False, summarize_pages: bool = True, num_sentences: int = 3) -> List[str]:\n",
    "    snippets = []\n",
    "    if results.get(\"answerBox\"):\n",
    "        answer_box = results.get(\"answerBox\", {})\n",
    "        if answer_box.get(\"answer\"):\n",
    "            snippets.append(f\"The answer is {answer_box.get('answer')}\")\n",
    "        elif answer_box.get(\"snippet\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippet')}\")\n",
    "        elif answer_box.get(\"snippetHighlighted\"):\n",
    "            snippets.append(f\"The answer might be in the snippet: {answer_box.get('snippetHighlighted')}\")\n",
    "\n",
    "    if results.get(\"knowledgeGraph\"):\n",
    "        kg = results.get(\"knowledgeGraph\", {})\n",
    "        title = kg.get(\"title\")\n",
    "        entity_type = kg.get(\"type\")\n",
    "        if entity_type:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {entity_type}.\")\n",
    "        description = kg.get(\"description\")\n",
    "        if description:\n",
    "            snippets.append(f\"Knowledge Graph Results: {title}: {description}.\")\n",
    "        for attribute, value in kg.get(\"attributes\", {}).items():\n",
    "            snippets.append(f\"Knowledge Graph Results: {title} {attribute}: {value}.\")\n",
    "\n",
    "    if scrape_pages:\n",
    "        tasks = []\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            task = asyncio.create_task(scrape_page_async(result[\"link\"], summarize_page=summarize_pages, num_sentences=num_sentences))\n",
    "            tasks.append(task)\n",
    "        summarized_pages = await asyncio.gather(*tasks)\n",
    "        for i, page in enumerate(summarized_pages):\n",
    "            result = results[result_key_for_type[search_type]][i]\n",
    "            if page:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\nSummarized Page: {page}\")\n",
    "            else:\n",
    "                snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "    else:\n",
    "        for result in results[result_key_for_type[search_type]][:k]:\n",
    "            snippets.append(f\"Title: {result.get('title', '')}\\nSource:{result['link']}\\nSnippet: {result.get('snippet', '')}\\n\")\n",
    "\n",
    "    if len(snippets) == 0:\n",
    "        return [\"No good Google Search Result was found\"]\n",
    "    return snippets\n",
    "\n",
    "import requests\n",
    "\n",
    "def can_embed_url(url: str):\n",
    "    response = requests.head(url)\n",
    "\n",
    "    # Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "    x_frame_options = response.headers.get('X-Frame-Options')\n",
    "    csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "    return not (x_frame_options or ('frame-ancestors' in csp if csp else False))\n",
    "\n",
    "def extract_entity_url_and_image(search_results: dict, image_results: dict):\n",
    "    # Only get the first top url and image_url\n",
    "    res = {}\n",
    "    if search_results.get(\"knowledgeGraph\"):\n",
    "        result = search_results.get(\"knowledgeGraph\", {})\n",
    "        if result.get(\"descriptionSource\") == \"Wikipedia\":\n",
    "            ref_url = result.get(\"descriptionLink\")\n",
    "            res[\"url\"] = ref_url\n",
    "\n",
    "    for result in search_results[result_key_for_type[\"search\"]][:k]:\n",
    "        if \"url\" not in res and result.get(\"link\") and can_embed_url(result.get(\"link\")):\n",
    "            res[\"url\"] = result.get(\"link\")\n",
    "            break\n",
    "\n",
    "    if image_results is None:\n",
    "        return res\n",
    "    \n",
    "    for result in image_results[result_key_for_type[\"images\"]][:k]:\n",
    "        if \"image_url\" not in res and result.get(\"imageUrl\"):\n",
    "            res[\"image_url\"] = result.get(\"imageUrl\")\n",
    "            break\n",
    "\n",
    "    return res\n",
    "\n",
    "async def search_url_for_entity_async(query: str):\n",
    "    async def inner_search(query:str): \n",
    "        search_task = asyncio.create_task(serper_search_async(\n",
    "            search_term=query,\n",
    "            gl=gl,\n",
    "            hl=hl,\n",
    "            num=k,\n",
    "            tbs=tbs,\n",
    "            search_type=\"search\",\n",
    "        ))\n",
    "\n",
    "        image_search_task = None if \"definition\" in query else asyncio.create_task(serper_search_async(\n",
    "            search_term=query,\n",
    "            gl=gl,\n",
    "            hl=hl,\n",
    "            num=k,\n",
    "            tbs=tbs,\n",
    "            search_type=\"images\",\n",
    "        ))\n",
    "\n",
    "        tasks = [search_task]\n",
    "        if image_search_task:\n",
    "            tasks.append(image_search_task)\n",
    "\n",
    "        search_results, image_results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        return extract_entity_url_and_image(search_results, image_results)\n",
    "    \n",
    "    res = await inner_search(query)\n",
    "    print(res)\n",
    "    if \"url\" not in res:\n",
    "        res = await inner_search(query + \" wiki\") # fallback search using wiki\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientResponseError",
     "evalue": "400, message='Bad Request', url=URL('https://google.serper.dev/images')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow is chatgpt doing medium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 144\u001b[0m, in \u001b[0;36msearch_url_for_entity_async\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    140\u001b[0m     search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n\u001b[0;32m--> 144\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m inner_search(query)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36msearch_url_for_entity_async.<locals>.inner_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_search_task:\n\u001b[1;32m    138\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(image_search_task)\n\u001b[0;32m--> 140\u001b[0m search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mserper_search_async\u001b[0;34m(search_term, search_type, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://google.serper.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m---> 33\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m search_results\n",
      "File \u001b[0;32m~/mambaforge/envs/tosg/lib/python3.11/site-packages/aiohttp/client_reqrep.py:1062\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1065\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1066\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1067\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 400, message='Bad Request', url=URL('https://google.serper.dev/images')"
     ]
    }
   ],
   "source": [
    "await search_url_for_entity_async(\"how is chatgpt doing medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what has been so revolutionary in the last 10 years, I would 15 years and thinking about the internet, I would say things like, hopefully I'm not saying anything ridiculous, but everything from Wikipedia to Twitter., So like these kind of websites, not so much AI, but like I would expect to see some kind of big productivity increases from just the connectivity between people and the access to more information., Yeah, well, so that's another area I've done quite a bit of research on actually, is these free goods like Wikipedia, Facebook, Twitter, Zoom. We're actually doing this in person, but almost everything else I do these days is online. The interesting thing about all those is most of them have a price of zero., What do you pay for Wikipedia? Maybe like a little bit for the electrons to come to your house. Basically zero, right? Take a small pause and say, I donate to Wikipedia. Often you should too. It's good for you, yeah. So, but what does that do mean for GDP? GDP is based on the price and quantity, of all the goods, things bought and sold. If something has zero price, you know how much it contributes to GDP? To a first approximation, zero. So these digital goods that we're getting more and more of, we're spending more and more hours a day consuming stuff off of screens, little screens, big screens,\n",
      " that doesn't get priced into GDP. It's like they don't exist. That doesn't mean they don't create value. I get a lot of value from watching cat videos and reading Wikipedia articles and listening to podcasts, even if I don't pay for them. So we've got a mismatch there. Now, in fairness, economists,\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"GDP\",\n",
      "      \"definition\": \"Total value of goods, services produced in a country\",\n",
      "      \"search_keyword\": \"Gross Domestic Product + concept\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='GDP', definition='Total value of goods, services produced in a country', search_keyword='Gross Domestic Product + concept')]\n",
      "entities=[Entity(name='GDP', definition='Total value of goods, services produced in a country', search_keyword='Gross Domestic Product + concept')]\n"
     ]
    },
    {
     "ename": "ClientResponseError",
     "evalue": "400, message='Bad Request', url=URL('https://google.serper.dev/images')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entities \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mentities:\n\u001b[0;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(entities\u001b[38;5;241m.\u001b[39msearch_keyword)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[1;32m      8\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_url_for_entity_async(entities\u001b[38;5;241m.\u001b[39msearch_keyword \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m wiki\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 144\u001b[0m, in \u001b[0;36msearch_url_for_entity_async\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    140\u001b[0m     search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n\u001b[0;32m--> 144\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m inner_search(query)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36msearch_url_for_entity_async.<locals>.inner_search\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_search_task:\n\u001b[1;32m    138\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(image_search_task)\n\u001b[0;32m--> 140\u001b[0m search_results, image_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extract_entity_url_and_image(search_results, image_results)\n",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m, in \u001b[0;36mserper_search_async\u001b[0;34m(search_term, search_type, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://google.serper.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mparams) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m---> 33\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m search_results\n",
      "File \u001b[0;32m~/mambaforge/envs/tosg/lib/python3.11/site-packages/aiohttp/client_reqrep.py:1062\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1062\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1065\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1066\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1067\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 400, message='Bad Request', url=URL('https://google.serper.dev/images')"
     ]
    }
   ],
   "source": [
    "test_transcript = generate_test_input()\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "print(res)\n",
    "for entities in res.entities:\n",
    "    res = await search_url_for_entity_async(entities.search_keyword)\n",
    "    if \"url\" not in res:\n",
    "        res = await search_url_for_entity_async(entities.search_keyword + \" wiki\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just go for the best intuitive way that works the best\n",
    "\n",
    "Pipeline\n",
    "1. Check if page can be embed\n",
    "2. Check if url is accurate for definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if page can be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page can be embedded.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the page you want to check\n",
    "url = 'https://en.wikipedia.org/wiki/Borat_Sagdiyev'\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.head(url)\n",
    "\n",
    "# Check the headers for 'X-Frame-Options' or 'Content-Security-Policy'\n",
    "x_frame_options = response.headers.get('X-Frame-Options')\n",
    "csp = response.headers.get('Content-Security-Policy')\n",
    "\n",
    "if x_frame_options or ('frame-ancestors' in csp if csp else False):\n",
    "    print(\"The page cannot be embedded.\")\n",
    "else:\n",
    "    print(\"The page can be embedded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Touring machines epitomize the quintessence of computational esotericism, manipulating symbols on a tape according to a tableau of rules. These automata traverse the tape's aleph-null segments, effectuating state transitions within a discrete, preternatural milieu. Ineffably, they delineate the demarcation of decidability and recursively enumerable conundrums.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Turing machines\",\n",
      "      \"definition\": \"Abstract machines that manipulate symbols on a tape.\",\n",
      "      \"search_keyword\": \"Turing machines + concept\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Aleph-null\",\n",
      "      \"definition\": \"Smallest infinity, size of natural numbers set.\",\n",
      "      \"search_keyword\": \"Aleph-null + concept\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Recursively enumerable\",\n",
      "      \"definition\": \"Set whose elements can be listed by an algorithm.\",\n",
      "      \"search_keyword\": \"Recursively enumerable + concept\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Turing machines', definition='Abstract machines that manipulate symbols on a tape.', search_keyword='Turing machines + concept'), Entity(name='Aleph-null', definition='Smallest infinity, size of natural numbers set.', search_keyword='Aleph-null + concept'), Entity(name='Recursively enumerable', definition='Set whose elements can be listed by an algorithm.', search_keyword='Recursively enumerable + concept')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Turing machines', definition='Abstract machines that manipulate symbols on a tape.', search_keyword='Turing machines + concept'), Entity(name='Aleph-null', definition='Smallest infinity, size of natural numbers set.', search_keyword='Aleph-null + concept'), Entity(name='Recursively enumerable', definition='Set whose elements can be listed by an algorithm.', search_keyword='Recursively enumerable + concept')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test mispelling\n",
    "test_transcript = \"\"\"Touring machines epitomize the quintessence of computational esotericism, manipulating symbols on a tape according to a tableau of rules. These automata traverse the tape's aleph-null segments, effectuating state transitions within a discrete, preternatural milieu. Ineffably, they delineate the demarcation of decidability and recursively enumerable conundrums.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enceladus, an enigmatic spheroid ensconced within Saturn's magniloquent rings, exudes cryptic cryovolcanic plumes. These plumes are a melange of volatile compounds, festooning the E-ring with a diaphanous, icy effulgence. Amidst the celestial ballet, Enceladus pirouettes, a harbinger of astrobiological enigmas and cosmochemical perplexities.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Enceladus\",\n",
      "      \"definition\": \"Moon of Saturn with geysers and possible subsurface ocean\",\n",
      "      \"search_keyword\": \"Enceladus + moon\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cryovolcanism\",\n",
      "      \"definition\": \"Volcanic activity in icy celestial bodies\",\n",
      "      \"search_keyword\": \"Cryovolcanism + concept\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"E-ring\",\n",
      "      \"definition\": \"Outermost ring of Saturn, made of ice particles\",\n",
      "      \"search_keyword\": \"Saturn's E-ring + celestial feature\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Enceladus', definition='Moon of Saturn with geysers and possible subsurface ocean', search_keyword='Enceladus + moon'), Entity(name='Cryovolcanism', definition='Volcanic activity in icy celestial bodies', search_keyword='Cryovolcanism + concept'), Entity(name='E-ring', definition='Outermost ring of Saturn, made of ice particles', search_keyword=\"Saturn's E-ring + celestial feature\")]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Enceladus', definition='Moon of Saturn with geysers and possible subsurface ocean', search_keyword='Enceladus + moon'), Entity(name='Cryovolcanism', definition='Volcanic activity in icy celestial bodies', search_keyword='Cryovolcanism + concept'), Entity(name='E-ring', definition='Outermost ring of Saturn, made of ice particles', search_keyword=\"Saturn's E-ring + celestial feature\")])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test rare terms\n",
    "test_transcript = \"\"\"Enceladus, an enigmatic spheroid ensconced within Saturn's magniloquent rings, exudes cryptic cryovolcanic plumes. These plumes are a melange of volatile compounds, festooning the E-ring with a diaphanous, icy effulgence. Amidst the celestial ballet, Enceladus pirouettes, a harbinger of astrobiological enigmas and cosmochemical perplexities.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Marble Caves of Patagonia, a rare and ethereal natural wonder, are often overshadowed by more renowned landmarks yet offer an otherworldly beauty to those few who navigate their secluded azure chambers.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Marble Caves of Patagonia\",\n",
      "      \"definition\": \"Stunning geological formations in Chile\",\n",
      "      \"search_keyword\": \"Marble Caves of Patagonia + natural wonder\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Marble Caves of Patagonia', definition='Stunning geological formations in Chile', search_keyword='Marble Caves of Patagonia + natural wonder')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Marble Caves of Patagonia', definition='Stunning geological formations in Chile', search_keyword='Marble Caves of Patagonia + natural wonder')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test unofficial names\n",
    "test_transcript = \"\"\"The Marble Caves of Patagonia, a rare and ethereal natural wonder, are often overshadowed by more renowned landmarks yet offer an otherworldly beauty to those few who navigate their secluded azure chambers.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex: Did you ever read about Hypatia in Alexandria? Her story is quite the example of schadenfreude among her peers. Jamie: True, I've always found it intriguing how people's limerence can lead to dramatic events. On a lighter note, have you heard the adage, \"A bird in the hand is worth two in the bush\"? Alex: Oh definitely, it's quite quotidian in its wisdom. Speaking of rare things, I was reading about Oymyakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbuktu someday. It's almost like a real-life cacophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Day celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolatry practices, they have such deep roots. Alex: Speaking of the world, the Bilderberg Meeting's recent convention had some interesting discussions. It's like a modern-day triumvirate in some ways.\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hypatia\",\n",
      "      \"definition\": \"Female philosopher and mathematician in ancient Alexandria\",\n",
      "      \"search_keyword\": \"Hypatia of Alexandria + person\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Schadenfreude\",\n",
      "      \"definition\": \"Pleasure derived from another's misfortune\",\n",
      "      \"search_keyword\": \"Schadenfreude + definition\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Limerence\",\n",
      "      \"definition\": \"State of intense romantic infatuation\",\n",
      "      \"search_keyword\": \"Limerence + definition\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hypatia', definition='Female philosopher and mathematician in ancient Alexandria', search_keyword='Hypatia of Alexandria + person'), Entity(name='Schadenfreude', definition=\"Pleasure derived from another's misfortune\", search_keyword='Schadenfreude + definition'), Entity(name='Limerence', definition='State of intense romantic infatuation', search_keyword='Limerence + definition')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Hypatia', definition='Female philosopher and mathematician in ancient Alexandria', search_keyword='Hypatia of Alexandria + person'), Entity(name='Schadenfreude', definition=\"Pleasure derived from another's misfortune\", search_keyword='Schadenfreude + definition'), Entity(name='Limerence', definition='State of intense romantic infatuation', search_keyword='Limerence + definition')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test transcript from chatgpt vtt\n",
    "test_transcript = \"\"\"Alex: Did you ever read about Hypatia in Alexandria? Her story is quite the example of schadenfreude among her peers. Jamie: True, I've always found it intriguing how people's limerence can lead to dramatic events. On a lighter note, have you heard the adage, \"A bird in the hand is worth two in the bush\"? Alex: Oh definitely, it's quite quotidian in its wisdom. Speaking of rare things, I was reading about Oymyakon. Can you imagine living in such cold? Jamie: Hard to even think about! Speaking of unique places, I'd love to visit Timbuktu someday. It's almost like a real-life cacophony of culture and history. Alex: Absolutely. Switching topics, are you going to the Dyngus Day celebration? I hear it's a blast. Jamie: I'd love to! It's always fun to learn about and participate in traditions from around the world. Just like heliolatry practices, they have such deep roots. Alex: Speaking of the world, the Bilderberg Meeting's recent convention had some interesting discussions. It's like a modern-day triumvirate in some ways.\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex: Did you ever read about Hypatya in Alexandria? Her story is quite the example of shadenfrood among her peers. Jamie: True, I've always found it intriguing how people's lemerence can lead to dramatic events. On a lighter note, have you heard the adage, \"A bird in the hand is worth too in the bush\"? Alex: Oh definitely, it's quite quodidian in its wisdom. Speaking of rare things, I was reading about Oymiakon. Can you imagine living in such cold?\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"name\": \"Hypatia\",\n",
      "      \"definition\": \"Female philosopher, mathematician in ancient Alexandria\",\n",
      "      \"search_keyword\": \"Hypatia of Alexandria + person\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Schadenfreude\",\n",
      "      \"definition\": \"Pleasure derived from another's misfortune\",\n",
      "      \"search_keyword\": \"Schadenfreude + definition\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Limerence\",\n",
      "      \"definition\": \"State of intense romantic infatuation\",\n",
      "      \"search_keyword\": \"Limerence + definition\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "proactive_rare_word_agent_response entities=[Entity(name='Hypatia', definition='Female philosopher, mathematician in ancient Alexandria', search_keyword='Hypatia of Alexandria + person'), Entity(name='Schadenfreude', definition=\"Pleasure derived from another's misfortune\", search_keyword='Schadenfreude + definition'), Entity(name='Limerence', definition='State of intense romantic infatuation', search_keyword='Limerence + definition')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationEntities(entities=[Entity(name='Hypatia', definition='Female philosopher, mathematician in ancient Alexandria', search_keyword='Hypatia of Alexandria + person'), Entity(name='Schadenfreude', definition=\"Pleasure derived from another's misfortune\", search_keyword='Schadenfreude + definition'), Entity(name='Limerence', definition='State of intense romantic infatuation', search_keyword='Limerence + definition')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noised up version, mispelling, etc\n",
    "test_transcript = \"\"\"Alex: Did you ever read about Hypatya in Alexandria? Her story is quite the example of shadenfrood among her peers. Jamie: True, I've always found it intriguing how people's lemerence can lead to dramatic events. On a lighter note, have you heard the adage, \"A bird in the hand is worth too in the bush\"? Alex: Oh definitely, it's quite quodidian in its wisdom. Speaking of rare things, I was reading about Oymiakon. Can you imagine living in such cold?\n",
    "\"\"\"\n",
    "\n",
    "print(test_transcript)\n",
    "res = run_proactive_rare_word_agent_and_definer(test_transcript, [])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tosg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
